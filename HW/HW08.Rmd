---
title: "HW08"
author: "Zach White"
date: "November 4, 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
#data(UScrime)
data = read.table("crime.dat", header = TRUE)
set.seed(9548)
```

# Problem 9.3

## Part A
```{r Part A}
g = n = nrow(data)

nu.0 = 2
sigma2.0 = 1
y = data$y
X = as.matrix(data[,-1])
n.samp = 10000

## Sigma2
p = ncol(X)
Hg = (g/(g+1)) * X %*% solve(t(X) %*% X) %*% t(X)
In = diag(1,n)
SSRg = t(y) %*% (In - Hg) %*% y

sigma2 = 1 / rgamma(n.samp, (nu.0 + n)/2, (nu.0*sigma2.0 + SSRg)/2)
## Betas
var.beta = (g/(g+1)) * ( solve(t(X) %*% X) )
mean.beta = (g/(g+1)) *(solve(t(X) %*% X) %*% t(X) %*% y)
E = matrix(rnorm(n.samp*p,0,sqrt(sigma2)),n.samp,p)
beta = t( t(E %*% chol(var.beta)) + c(mean.beta))

names(beta) = colnames(X)
apply(beta,2,quantile,c(.025,.975))
apply(beta,2,mean)
ols = lm(y~.,data= data)
coef(ols)
beta.hat.ols = solve(t(X) %*% X) %*% t(X) %*% y
#longhand.pred = as.matrix(test.X) %*% beta.hat.ols
```

Some comments on the comparison.  Specifically, which variabes seem very predictive.  Look at the credible intervals.

## Part B
```{r}
train = sample(n,n/2)
train.set = data[train,]
test.set = data[-train,]
train.y = train.set$y
train.X = train.set[,-1]
test.y = test.set$y
test.X = test.set[,-1]

# OLS on training, predicted values
train.lm = lm(y~., data = train.set)
pred.y.ols =predict.lm(train.lm,test.X)
# OR longhand
train.lm.beta0 = train.lm$coefficients[1]
train.lm.beta = train.lm$coefficients[-1]
long.pred = train.lm.beta0+ (as.matrix(test.X) %*% train.lm.beta)
train.X = as.matrix(train.X)
test.X = as.matrix(test.X)
beta.hat.ols = solve(t(train.X) %*% train.X) %*% t(train.X) %*% train.y
longhand.pred = as.matrix(test.X) %*% beta.hat.ols

long.ols.error = sum((test.y-longhand.pred)^2)
ols.error = sum((test.y-pred.y.ols)^2)

# Bayesian on training, 
g = n = nrow(train.set)
train.X = as.matrix(train.X)
test.X = as.matrix(test.X)

nu.0 = 2
sigma2.0 = 1

n.samp = 10000

## Sigma2
p = ncol(train.X)
Hg = (g/(g+1)) * train.X %*% solve(t(train.X) %*% train.X) %*% t(train.X)
In = diag(1,n)
SSRg = t(train.y) %*% (In - Hg) %*% train.y

sigma2 = 1 / rgamma(n.samp, (nu.0 + n)/2, (nu.0*sigma2.0 + SSRg)/2)
## Betas
var.beta = (g/(g+1)) * ( solve(t(train.X) %*% train.X) )
mean.beta = (g/(g+1)) *(solve(t(train.X) %*% train.X) %*% t(train.X) %*% train.y)
E = matrix(rnorm(n.samp*p,0,sqrt(sigma2)),n.samp,p)
beta = t( t(E %*% chol(var.beta)) + c(mean.beta))
mean.beta = apply(beta,2,mean)

pred.y.bayes = test.X %*% mean.beta

bayes.error = sum((test.y - pred.y.bayes)^2)

par(mfrow = c(1,3))
plot(test.y,pred.y.ols, main = "OLS with int")
plot(test.y,longhand.pred, main = "OLS no int")
plot(test.y,pred.y.bayes,main = "Bayes")

long.ols.error
ols.error
bayes.error
```
Note: I report three errors because in the initial data, we assume that the data has been centered and scaled so it has a mean of zero and a variance of 1.  When data are used like this, a regression model doesn't include a term for $\beta_0$ because it should be 0 in theory.  However, even in the initial model, the coefficient for $\beta_0 > 0$, and so I chose to include it to see the discrepancies.  Also, something of note is that since we are taking random samples in our cross validation, there is a relatively high probability that each sample won't have a mean of 0, and thus not including $\beta_0$ isn't necessarily a great idea.  For this reason, I will report all three errors both in this problem and the next.

When we perform this analysis, we get our Bayes coefficients and the ones from OLS without an intercept are pretty similar, and they are much lower than if we were to include an intercept.  However, this might just be due to our random sample, which is why we have our next question.

## Part C
```{r, cache = TRUE}
n.tot = nrow(data)
n.iter = 10000
error.int.ols = error.noint.ols = error.bayes = rep(0,n.iter)
# Prior Values
g = n = nrow(data) /2
nu.0 = 2
sigma2.0 = 1
n.samp = 10000

for(i in 1:n.iter){
  train = sample(n.tot,n.tot/2)
  train.set = data[train,]
  test.set = data[-train,]
  train.y = train.set$y
  train.X = as.matrix(train.set[,-1])
  test.y = test.set$y
  test.X = as.matrix(test.set[,-1])
  # OLS with intercept
  train.lm = lm(y~.,data = train.set)
  y.int.ols = predict.lm(train.lm,as.data.frame(test.X))
  # OLS without intercept
  beta.hat.ols = solve(t(train.X) %*% train.X) %*% t(train.X) %*% train.y
  y.noint = test.X %*% beta.hat.ols
  # Bayes
  ## Sigma2
  p = ncol(train.X)
  Hg = (g/(g+1)) * train.X %*% solve(t(train.X) %*% train.X) %*% t(train.X)
  In = diag(1,n)
  SSRg = t(train.y) %*% (In - Hg) %*% train.y
  
  sigma2 = 1 / rgamma(n.samp, (nu.0 + n)/2, (nu.0*sigma2.0 + SSRg)/2)
  ## Betas
  var.beta = (g/(g+1)) * ( solve(t(train.X) %*% train.X) )
  mean.beta = var.beta %*% t(train.X) %*% train.y
  E = matrix(rnorm(n.samp*p,0,sqrt(sigma2)),n.samp,p)
  beta = t( t(E %*% chol(var.beta)) + c(mean.beta))
  mean.beta = apply(beta,2,mean)
  
  pred.y.bayes = test.X %*% mean.beta
  
  error.bayes[i] = sum((test.y - pred.y.bayes)^2)
  error.int.ols[i] = sum((test.y - y.int.ols)^2)
  error.noint.ols[i] = sum((test.y - y.noint)^2)
  
}

mean(error.bayes)
mean(error.int.ols)
mean(error.noint.ols)
quantile(error.bayes,c(.025,.975))
quantile(error.int.ols,c(.025,.975))
quantile(error.noint.ols,c(.025,.975))

preds = cbind(pred.y.bayes, y.noint)
names(preds) = c("bayes","ols")
#ggplot(data = preds, aes(x = bayes,y = ols)) + geom_point()

```

Ater repeated sampling, we can see that the Bayesian regression actually has the lowest predictive error, followed by the OLS without the intercept and then the OLS with the intercept.