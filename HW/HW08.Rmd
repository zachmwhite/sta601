---
title: "HW08"
author: "Zach White"
date: "November 4, 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
#data(UScrime)
data = read.table("crime.dat", header = TRUE)
```

# Problem 9.3

## Part A
```{r Part A}
g = n = nrow(data)

nu.0 = 2
sigma2.0 = 1
y = data$y
X = as.matrix(data[,-1])
n.samp = 10000

## Sigma2
p = ncol(X)
Hg = (g/(g+1)) * X %*% solve(t(X) %*% X) %*% t(X)
In = diag(1,n)
SSRg = t(y) %*% (In - Hg) %*% y

sigma2 = 1 / rgamma(n.samp, (nu.0 + n)/2, (nu.0*sigma2.0 + SSRg)/2)
## Betas
var.beta = (g/(g+1)) * ( solve(t(X) %*% X) )
mean.beta = (g/(g+1)) *(solve(t(X) %*% X) %*% t(X) %*% y)
E = matrix(rnorm(n.samp*p,0,sqrt(sigma2)),n.samp,p)
beta = t( t(E %*% chol(var.beta)) + c(mean.beta))

names(beta) = colnames(X)
apply(beta,2,quantile,c(.025,.975))
apply(beta,2,mean)
ols = lm(y~.,data= data)
coef(ols)
```

Some comments on the comparison.  Specifically, which variabes seem very predictive.  Look at the credible intervals.

## Part B
```{r}
train = sample(n,n/2)
train.set = data[train,]
test.set = data[-train,]

# OLS on training, predicted values

# Bayesian on training, 

```
## Part C